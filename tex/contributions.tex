\chapter{Contributions}
\label{sec:contributions}
\fixme{(Figure: Fields in which we need instant feedback on appearance: 3d printing, artist feedback, quality control, meat)}

In this section, after introducing relevant theory and related work, we finally discuss the different contributions crated over the course of the PhD studies. The goal of this section is to discuss the individual contributions in the light of the PhD goal. We remand to the text of the individual publications in Appendices~\ref{sec:firstcontribution}-\ref{sec:lastcontribution} for the full details.

As we discussed in Chapters~\ref{sec:background} and~\ref{sec:related}, many efforts have been put forward by the graphics community towards interactive physically based rendering. We attempted to report the major efforts in this regard in Chapter~\ref{sec:related}, categorizing the individual technique into three main categories, namely caching, precomputation and filtering.  

We argue that there is a need in the industry for photorealistic accurate interactive rendering, i.e. the top-left area of Figure~\ref{fig:main_diagram}. In many fields, people need immediate feedback on on the aspect of the final product. Some examples include visual inspection of produced parts, preview of 3D printed objects, artistic iterations for movie scenes, and prediction of the outcomes of an industrial process. We showed in Figure~\ref{fig:main_results} how our contributions contribute in various parts of this wide spectrum of needs.

We will start by Contribution~\ref{sec:juice}, a first case study on why we need both fast and accurate rendering, in the form predicting the appearance of cloudy apple juice given production parameters. In Contribution~\ref{sec:glass} we discuss the definition of photorealistic rendering, and how good a rendering should be to allow accurate parameter estimation. After this, we start discussing the interactive techniques we contributed with. In Contribution~\ref{sec:interactivedirsss} we discuss an interactive method improving physical accuracy for translucent materials, in the form of a caching and filtering technique that avoids any precomputation. We will continue with Contribution~\ref{sec:srt}, a caching and filtering technique leveraging recent innovations in interactive ray tracing to solve a widespread problem in real time graphics, temporal stability. In Contribution~\ref{sec:vrbrdf} we use physically based materials in a virtual reality environment. We will conclude by discussing some future direction we could expand our work in the future.

\section{Defining photorealistic rendering}
\label{sec:definingphoto}
\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{}c@{}}
	 \includegraphics[width=0.4\columnwidth]{figures/teaser_render.png} &
	 \includegraphics[width=0.4\columnwidth]{figures/ref_img.jpg}  \\
	rendering & photograph \\
\end{tabular}
\caption{Cloudy apple juice photographed and rendered the appearance model from Contribution~\ref{sec:juice}. In the model, we inferred apple particle concentration (0.8 g/l) and apple storage period (4 days) to match the photograph.} %The red rectangle shows where we estimated RMSE in Table \ref{table:quant}.}
\label{fig:juicecomparison}
\end{figure}

We start our discussion from Contributions~\ref{sec:juice} and \ref{sec:glass}. Our first discussion point is about what defines photorealistic rendering. In literature, the emphasis is often onto creating rendering models and techniques that approximate and arbitrary radiometric process. However, not much emphasis is put into validating the developed models on a physically based approach, like we do in Figure~\ref{fig:juicecomparison}, but relying instead on path traced references. Our first contributions~\ref{sec:glass} and~\ref{sec:juice} set out to test the limits of physically based rendering, testing how close a rendering using path tracing can get to real images. 

These two investigations led to a number of interesting insight on physically based rendering. In our first contribution~\ref{sec:juice}, we created a new appearance model to predict the appearance of cloudy apple juice. The goal here is to be able to predict the final appearance of apple juice by changing a production parameters, such as the concentration of the juice or the type of environment the apples are pressed. For example, a juice company would like to change the concentration of apple in the juice, and would like to ensure that the final product is still visually appeasing for the customer. Given that the space of production parameters is potentially huge, it is important to give immediate feedback using accurate physically based rendering so that various possible parameter configurations can be tested quickly. This comparison framework could be used in both directions: as prediction for the final appearance, but also to measure the production parameters of an unknown sample. A first insight is that if the scene is carefully set and calibrated, a comparison is definitely possible, and actual production parameters can be guessed. The biggest issue is in carefully setting and calibrating the scene. In this first proof of concept, we placed the objects and the light manually, using reasonable estimates for their appearance parameters. There is a number of different challenges in comparing pictures and renderings, mostly related to the scene. Subtle changes in the scene can lead to big differences in direct comparisons, especially when the material influences its surroundings, such and in the case of cloudy apple juice. See the light caustic next to the glass in Figure~\ref{fig:juicecomparison}. In this initial proof of concept, we simply compared a patch to get a parameter estimate.

\begin{figure}
\begin{tabular}{@{}c@{}c@{}}
	 \includegraphics[height=4.3cm]{figures/comparison} & \hspace{2em}
	 \includegraphics[height=4.3cm]{figures/glass_bowl_analysis_by_synthesis}  \\
\end{tabular}
\caption{To the left, comparing quantitatively renderings (top row) with real images (mid row). Error is shown in the bottom row. On the right, estimating the absorption parameter $\sigma_a$ for the glass bowl. Each coefficient was estimated independently. Each dot in the graph corresponds to a rendered image. }
\label{fig:glasscomparison}
\end{figure}
The role of our proof of concept in Contribution~\ref{sec:juice} is dual. First, it tells us that it is important to validate renderings with photographs, to ensure that the rendering matches the appearance in the real world. Secondly, it tells us that this comparison has to be achieved fast, so that multiple parameters can be tested, or even estimated, depending on the application. We will discuss the first aspect in the rest of this section, referring to the next section for a more complete discussion on fast rendering of scattering materials. In our main contribution~\ref{sec:glass}, we strive to improve upon the previous results of comparing rendering with images, for a different application. In this case, we are comparing images of glass objects. As it is true for scattering materials, the appearance of glass objects is greatly influenced by the surrounding scene, making the scene estimation even more important for this application. We use a full pipeline to accurately estimate the scene, scan glass objects with CT scanners and place them in the scene for final rendering. Our main contribution of this work is actually the pipeline, that allows researchers to compare pictures and renderings of glass objects, getting a quantitative comparison at the end. This allows researchers to experiment with each step of the pipeline, improving upon the state of the art techniques used. As we can see from Figure~\ref{fig:glasscomparison} on the left, we are able to quantitatively compare images of glass objects with pictures, something that as never been done before. This ability of qualitatively compare images and rendering would allow in the future to further improve existing techniques in acquisition, rendering and reconstruction. Given our improved reconstruction results, we can estimate material properties much more accurately than our previous contribution. In this particular case, we measure both the relative index of refraction $\eta$ and the spectral absorption coefficient $\sigma_a$ for each of the glass objects, as we can see in Figure~\ref{fig:glasscomparison} to the right. Note that each point of the graph is a rendering, so it becomes essential to generate a great number of images to estimate the material properties. Since we want accuracy, we need to use unbiased path tracing, that we accelerate using the GPU and execute at lower resolutions to achieve fast and accurate rendering of these images. 

\section{Interactive rendering of scattering media}
\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{$\,$}c@{}c@{}c@{}}
& directional dipole, 6 fps & standard dipole and VPLs \\
\begin{sideways}\hspace*{1.5em}our method\end{sideways} &
\includegraphics[width=0.43\columnwidth]{figures/candle_holder_directional_6fps.png} &
\includegraphics[width=0.43\columnwidth]{figures/candle_holder_jensen_converged.png} \\[-4pt]
\begin{sideways}\hspace*{1.7em}ray tracer\end{sideways} &
\includegraphics[width=0.43\columnwidth]{figures/scene_comparison_optix_6fps.png} &
\includegraphics[width=0.43\columnwidth]{figures/scene_comparison_converged.png} \\[-0.5ex]
& directional dipole, 6 fps & directional dipole, reference \\[-1ex]
\end{tabular}
\caption{Equal time comparison (left column) of our method with the reference method and qualitative comparison with diffuse subsurface scattering (upper right) and the converged reference solution (lower right). The scene is lit by a point light in a white grapefruit candle holder.} % Emerging light illuminates a diffuse Stanford Bunny on a tabletop.}
\label{fig:optixcomparison}
\end{figure}

After discussing photorealistic rendering in the previous section, in this section we start discussing our first technique that brings photorealistic rendering into the interactive domain. We also here take inspiration from Contribution~\ref{sec:juice}: in the previous section, we discussed the importance of physically based material. In this section, we present an example that allows us to achieve a better photorealistic look.

Our Contribution~\ref{sec:interactivedirsss} is a rasterization-based caching scheme to improve efficiency of existing techniques. The most important contribution in this technique is that allows rendering using \emph{directional} BSSRDFs like the directional dipole discussed in Section~\ref{sec:analyticalbssrdf}. Our technique allows to render with any BSSRDF analytical model that depends on $\vec{\omega}_i$, the direction of the incoming light. This allows more subtle scattering effects to be computed, accounting partially for single scattering, that in previous techniques needed to be added separately. See Figure~\ref{fig:optixcomparison}, top row, for a comparison between the standard dipole~\cite{Jensen2001} and the directional dipole~\cite{Frisvad2014}. Most of the interactive and real time techniques for rendering BSSRDFs assume that the BSSRDF is function of the distance between the point of incidence and emergence only. This can be exploited for different optimizations, like filtering, precomputation or tabulation, that are not feasible anymore when it comes to using a directional technique. Our technique is unique in handling this specific type of directional dipoles in the interactive domain. 

In the classification of different techniques presented in Chapter~\ref{sec:related}, our technique is mostly a caching technique, with some filtering required to assemble the final image. Our technique leverages the strengths of rasterization, storing progressive maps of scattered radiosity rendered from different directions around the object. We can now account for the directionality of the light in the computation, and in addition progressively store the intermediate result as soon as the light and the object do not change. We contribute with a fully interactive technique, that does not require neither precomputation nor texture parameterization. Given this features, we can apply this technique to procedural deformable objects, something that is generally quite difficult to achieve with precomputation techniques. In our technique, we contribute with an improved sampling scheme, that via our maps can sample radiosity always close to the light source, allowing light to propagate through objects and around sharp corners. This can be seen in particular in Figure~\ref{fig:optixcomparison}, in the top left image, where a point light source is places inside a candleholder made of a scattering material.  

As another contribution of our technique, we leverage our scattered radiosity maps to place VPLs on the surface of the objects. This allows to transfer emergent light onto other surfaces. An example can be seen in Figure~\ref{fig:optixcomparison}, where we transport the light from a point light inside an object on the outside, illuminating the bunny on the tabletop. In the same figure, we can see on how we successfully compare to a fully path traced simulation (bottom right) that in the same time we render our solution cannot achieve the same results, looking unconverged and noisy (bottom left).

To sum up, the take away home message of this technique is that improved physical models, such as the directional dipole, sometimes do not allow us to reuse previous work. So, we need to develop new techniques to achieve interactive result, so that we can include these more accurate effects. Moreover, the benefits of caching data in this case become particularly apparent, since they allow us to recycle the stored radiosity for another technique to use. 

\section{Interactive stable ray tracing}

\begin{figure}[t]
\begin{tabular}{@{}c@{}c@{}@{}c@{}}
	 \includegraphics[width=0.32\textwidth]{figures/ss_2x_rect_370_300_300_300_frame_211.png} &
		 \includegraphics[width=0.32\textwidth]{figures/ss_2x_taa_rect_370_300_300_300_frame_211.png} &
		  \includegraphics[width=0.32\textwidth]{figures/ss_32x_rect_370_300_300_300_frame_211.png} \\	 
Supersampling, 2 spp & Supersampling, 2 spp 	& Supersampling, 32 spp \\
  					 & + temporal antialiasing 	&  \\
sharpness: 0.7924 & sharpness: 0.5348 & sharpness: 0.6771  \\
	 \includegraphics[width=0.32\textwidth]{figures/srt_1_rect_370_300_300_300_frame_211.png} &
	 	 \includegraphics[width=0.32\textwidth]{figures/srt_1_ti_rect_370_300_300_300_frame_211.png} &
	  \includegraphics[width=0.32\textwidth]{figures/ss_32x_rect_370_300_300_300_frame_211.png}
 \\
Stable RT, 1 spp & Stable RT, 1 spp & Supersampling, 32 spp \\
 & + temporal integration &  \\
sharpness: 0.7085 & sharpness: 0.6060 & sharpness: 0.6771 \\[-1.5ex]
\end{tabular}
\caption{Different techniques applied to a frame in the Sponza video, equal time comparison. Sharpness is also shown (higher is sharper). Stable ray tracing offers a less noisy result (first column) compared to reference (third column). When temporal techniques are applied (second column) Stable ray tracing yields the sharper result.  }
\label{fig:sponza_video_frame}
\end{figure}
After dealing with subsurface scattering, we continue exploring the spectrum of interactive photorealistic techniques in Contribution~\ref{sec:srt}. As in the previous section, we use caching as a technique to improve existing physically based techniques. In this contribution, in particular, we discuss the problem of achieving renderings that are temporally stable. In our case, we discuss a possible solution applicable to interactive ray tracing.  

When going to a interactive or real time ray tracing technique, the number of paths we can shoot per pixel becomes extremely limited, in the order or one or two. Also, the shading locations change every frame, causing a noisy image, both spatially and temporally. Existing techniques, namely temporal anti aliasing, can mitigate the problem, tough they generally introduce blur. in our technique, we recycle shading locations across frames. Shading always the same points, we improve temporal stability, while retaining sharpness. We can see the results in Figure~\ref{fig:sponza_video_frame}. In the result we can see how stable ray tracing yields a  sharper result than temporal antialiasing. The technique contributes as a rendering system to improve temporal stability: other existing techniques can be further applies on top to further improve shading quality. 

Another contribution of this technique is that it shows how we can leverage the strength of one interactive technique, namely ray tracing, against the most commonly used rasterization. Current hardware does not allow shading locations to be arbitrarily chosen within a pixel, relying on fixed patterns (such as in MSAA). In our technique, we allow shading locations to vary in screen space per pixel, while staying the same in world space. 

Finally, another advantage of this technique, in particular in a photorealistic rendering context, is that it allows to store shading information across frames, e.g. to store indirect illumination. This will allow in the future to extend the technique with other sort of data, to further improve the technique. This gives the take home message of this technique: we need generic, robust techniques that can be applied in a variety of situations, inexpensive and that can work together with existing techniques. We believe that stable ray tracing satisfies these characteristics. 

\section{Applying interactive photorealistic techniques}
\begin{figure}[t]
\centering
\begin{tabular}{@{}c@{}c@{}}
	 \includegraphics[height = 4.3cm]{figures/screen1_crop} &
		 \includegraphics[height = 4.3cm]{figures/person} \\[-2.5ex]
\end{tabular}
  \caption{Pictures illustrating our VR demo application, with an in-game screenshot (left) and a picture of the setup (right). }
  \label{fig:vrbrdfimage}
\end{figure}
In contribution~\ref{sec:vrbrdf}, we push physically based rendering even further, applying it to virtual reality. In this context, applications need to consistently perform at 90 frames per second or faster, to avoid issues for the users, e.g. dizziness or motion sickness. In our application, we modify the rendering pipeline of the Unity game engine  to include physically based BRDFs, in the discretized form of the MERL database, as described in Section~\ref{sec:empiricalbrdf}. This proof of concept was born as an inspecting tool to debug physically based materials, aided by the provided in game controllers (see Figure~\ref{fig:vrbrdfimage}). The application gives us a glimpse on how photorealistic materials are able to augment the immersiveness of the application, which is paramount in virtual reality applications.

\section{Discussion}
In the previous sections, we showed how our contribution overall contribute to the goal of expanding interactive technique towards a more physically accurate framework. The natural step of the topics discussed in this thesis is to further expand the techniques in photorealism, by including as an example more accurate physical models, but retaining the same performance level. In the following, we discuss some possible avenues of expansion of our work.

\textbf{Validating path tracing}. Some avenues are possible in continuing the work we initiated in Contributions~\ref{sec:juice} and \ref{sec:glass}. In particular, we would like to continue our initial goal of validating path tracing. This would require expanding the technique to be more accurate, in particular in regards to geometry. For a more complete validation, we would like to expand the technique to handle fully scattering materials instead of glass only. Other avenues of research are possible in regards of estimating parameters: scattering parameters ($\sigma_a$, $\sigma_s$ and $g$) are obvious candidates for a further expansion.

\textbf{Hybrid rasterization-ray tracing rendering techniques}. As for now, we focused on our technique to be exclusively rasterization or ray tracing based. Some potential improvements can be thought by combining the two techniques. For example, some could think of an extension of our stable ray tracing in Contribution~\ref{sec:srt} to support our technique from Contribution~\ref{sec:interactivedirsss}. In this particular case, the scattered radiosity maps can be replaced by the stable ray traced points. The rasterization of the light G-buffer would still have to be executed, making this an hybrid technique.  Another example, in our virtual reality contribution~\ref{sec:vrbrdf}, would be to evaluate reflections via ray tracing, to properly include the overall contribution instead of an approximation via a precomputed probe.

\textbf{Virtual reality}. In recent year, virtual reality has become more prominent in real time applications. Given the hard constraints of virtual reality, the techniques we developed would need to be adapted to fit a virtual reality environment. In particular our stable ray tracing algorithm in Contribution~\ref{sec:srt} seems like a good fit for a virtual reality environment, that is particularly plagued by temporal stability issues. 

\textbf{Dataset generation for machine learning}. Interactive techniques for photorealistic rendering allow generating image data much faster than traditional techniques. This makes them particularly useful to generate synthetic data to train image based machine learning techniques.  