\chapter{Related work}
FIXME more connections to theory
FIXME connections to following chapter more clear
FIXME further explanations

\label{sec:related}

In this section, we present some related work on the overall topic of the thesis, namely physically based techniques brought into the interactive domain. We will make use of the selected background theory we presented in Chapter~\ref{sec:background}, to better discuss our contributions in Chapter~\ref{sec:contributions}. This section is not meant to be a complete survey on such techniques, but we will point at the most important contributions, referring to a number of individual papers and surveys for a more complete overview. We also refer to the related work of the individual contributions for a more detailed description of related work in the context of the individual paper. Finally, we will have a slight bias towards describing techniques that are similar to the one we present in Chapter~\ref{sec:contributions}, in order to better compare and emphasize our contributions.
With these assumptions in place, let us move on. 

As we mention in Chapter~\ref{sec:intro}, interactive rendering requires a compromise between how accurate the final rendering will be and how fast the technique will be. There are various examples in literature on the compromises that are needed to hit this trade-off. We identified three main broad approaches into which the various technique fall: caching, precomputation and filtering. In the first one, we use some intermediate data structures to accelerate computation or to reuse data. The second technique, given some assumptions about the materials or geometry, to precompute data to increase the overall speed of the algorithm. Finally, filtering technique improve the final quality of the image reconstructing information based on the available data. Note that these techniques are not mutually exclusive: each technique may imply a combination of caching, precomputation and filtering. In the following sections, we will describe for each approach various techniques that make mainly use of that approach to render accurate images. 

\section{Simplified physical models}

Discuss here simplifications to the theory? BRDF vs BSSRDF? Radius based, etc.?

\section{Caching}

The caching approach involves creating data structures to store additional data in a rendered scene, to make rendering faster. We can subdivide the techniques in this area into two broad categories: caching geometry and caching lighting.

A number of techniques caching geometry provide efficient data structures to traverse the scene. The most simple and widely used of these techniques is deferred shading REF, that rasterizes geometry, depth and positions into a highly optimized screen space data structure. This structure allows sampling of geometry in a local neighborhood and can be used to implement screen space techniques, such as ambient occlusion. Another technique is to create a traversal structure to efficiently implement ray tracing, as the bounding volume hierarchy described in section TODO. Data structures for interactive ray tracing is a field by itself, so we point out to a relevant survey by REF. In particular, we want to point out the paper by Karras [2013], the first to introduce the TRBVH, a particular structure that allows fast rebuilding of the BVH on the fly, and that it is implemented in OptiX, NVIDIA's ray tracing engine REF. Once a traversal structure for ray tracing is in place, various classical ray tracing algorithms can be efficiently implemented, such as recursive ray tracing, path tracing, or volumetric path tracing. Other data structures can be used, such as octrees REF or directly on the G-buffer itself.

When we consider light caching techniques, we also have screen space ones. These involve storing the radiance from a frame and reprojecting it to the next, then filling holes created through visibility mismatches. The radiance cache REF is a seminal paper in this regard. Other light caching techniques propagate illumination in the scene, storing it, then rendering it from the camera. Screen space techniques can be used to efficiently render also scattering media, by solving the extended rendering equation REF by sampling geometry in a local neighborhood. In photon mapping REF, photons are cast from light sources, bounced around the scene though ray tracing, then arranged into photon maps. Density estimation is then use to derive the illumination from the maps. The overall efficiency can be enhanced by building a data structure for fast gathering of nearby photons. Instant radiosity and VPL techniques use a similar approach, but use gathering instead of density estimation in order to estimate the final radiance value at the exit point. Each photon stored in the scene is treated as a virtual point light. A lot of literature deals with eliminating VPL singularities REF, adding visibility REF or generalizing them into virtual area lights REF.

Mixed techniques that combine geometry and light approximation are also possible, as in point based global illumination, where the scene is represented as a series of either points or surface elements (surfels). A hierarchy of surfels is then built. Surfels are then shader, then relevant ones are then rendered per each pixel, resolving indirect visibility. Other techniques such as instant radiosity approximate the scene ass a series of geometric patches, then precomputes the light transport in between patches for subsequent interactive rendering. 

Other techniques use efficient data structures to do a light transport simulation. These techniques are particular suitable to render participating media, but they can also be used to render traditional glossy bounce. In the case of participating media, these techniques use finite element technique to solve the radiative transfer equation on a discretized grid. Various improvement can be applied, such as using a adaptive octree data structure to propagate lighting. This particular techniques are used to obtain real time results, such as in the case of light propagation volumes REF or voxel cone tracing. 


\section{Pre-computation}
In this section, we deal with the aspect of precomputation. By introducing some limitation in our scene, we can precompute some data for efficient rendering. Limitations include static or mostly static geometry, static light, fixed materials or fixed view point. 

Relighting techniques are a first example in which we require camera and geometry to be fixed. In this case, we precompute and change lighting in the scene. These techniques are suitable in a preview environment, or to perform artistic tweaks in the final rendering. REF is an example of this technique used in movie rendering pipelines. 

If we assume static geometry only (light and view can freely change), precomputed radiance transfer techniques are currently used nowadays in interactive and real time applications. These techniques involve precomputing the radiance transfer at the surface for infinitely distant lights. Basis function are used to approximate lighting transfer at the surface, allowing slow preprocessing times but a fast evaluation via a dot product. The quality of the rendering depends on the number of basis functions used, though the memory and performance requirements increase as well. Spherical harmonics are the most commonly used baasis functions, but more complex ones are possible. Precomputed radiance transfer gives good result representing low frequency lighting changes across the scene. Still in the case of static geometry algorithm, radiosity discretizes the scene as a series of patches, then calculates the transfer between each patch. This allows fast illumination with changing lights and viewpoints, at the price of a quadratic memory increase depending on the number of patches. 

if we assume static light and geometry, the whole incoming illumination at a point can be cached, then used to re-light objects moving through the scene. Moreover, static illumination can be stored as an additional texture map, in a process called baking. Illuminatino can then be used in rendering to represent complex effect, with direct illumination added on top. 

\section{Filtering}

The last branch of techniques we discuss is filtering. Filtering approaches regard reconstructing the final appearance based on a limited set of samples. As in previous sections, we start by describing screen space techniques. A big area of research includes anti-aliasing techniques, that involve improving appearance of undersampled features in the scene. Important technique in this area include MSAA REF to improve geometric aliasing and other work REF to improve aliasing of specular. Temporal antialiasing technniques involve recycling color information from the previous frame via motion vectors, and combining with the current color distribution, to achieve antialiasing across time. More recently, new techniques have been developed to filter noisy one sample Monte Carlo simulations to achieve a smooth temporally stable result. Work in this area include advanced edge-awware bilateral filtering, temporal variance averaging, and machine-learning based filtering. Screen space techniques can be effectively used also in the case of scattering media, approximating the scattering process as a series of Gaussian filters. This effectively approximates the diffusion process using the standard dipole REF, leading to plausible realistic results. 

Moving into more advanced structures, irradiance caching techniques store the illumination at a limited set of points in the scene, or pixels, then interpolating illumination across the points to achieve overall results. The technique can be further enhanced to include spatial heuristics in order to avoid light leaking though objects. Another techcnique in global space is probe sampling, where precomputed radiance transfer is stored at fixed points in the scene, then the illumination is interpolated across the probes to allow lighting of both static and dynamic geometries. Interpolation can be done in many ways, including Voronoi 3D grids to allow arbitrary placements of probes.


As we have seen in this section, the space of interactive techniques that try to deliver a physically accurate result is huge. Most of these techniques rely on simplifications, assumptions and discretizations that allow fast rendering, sacrificing physical accuracy. In the net chapter, we will show how we improved upon current techniques to achieve a more physically accurate result.
