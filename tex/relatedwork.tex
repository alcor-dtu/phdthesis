\chapter{Related work}
%
\label{sec:related}
%
In this section, we present some related work on the overall topic of the thesis, namely physically based techniques brought into the interactive domain. We will make use of the background theory we presented in Chapter~\ref{sec:background}, to better discuss our contributions in Chapter~\ref{sec:contributions}. This section is not meant to be a complete survey of such techniques, but we will point to the most important contributions, referring to a number of individual papers and surveys for a more complete overview. In particular, we point to the survey by~\citet{Ritschel2012} for a good overview of various interactive techniques for global illumination. We also refer to the related work of the individual contributions in the appendices for more detailed literature in the context of each contribution. 

As mentioned in Chapter~\ref{sec:intro}, interactive rendering requires a compromise between accuracy and speed. There are various examples in literature on the compromises that are needed to improve this trade-off. Since the rendering time constraints are usually set, compromises are usually done in physical accuracy. We will start by discussing some of the assumptions that are usually employed in interactive rendering environments. 
%
\section{Typical approximations in physically based rendering}

One of the natural approaches towards achieving interactive photorealistic rendering is making some assumptions about our physical simulation. In principle, we can describe any light interaction by running a brute-force simulation of all the photons interacting with the particles of the material, then measuring the number of photons arriving at a set light sensor. Of course, running this sort of computation, even for extremely simple scenes, in reasonable times is not possible. In general, Monte Carlo path tracing~\cite{Kajiya1986} with extension to scattering media~\cite{Rushmeier1988} is considered a ground truth technique to generate reference images. Path tracing is an unbiased technique (i.e. converges always to the correct solution), but it gives extremely noisy results, especially for particular configurations of light and camera positions. To solve this problem, the offline rendering community proposed various improvements to reduce variance in path tracing, such as importance sampling~\cite{Kirk1991},multiple importance sampling~\cite{Veach1995}, Metropolis sampling~\cite{Veach1995}, Bidirectional path tracing~\cite{Veach1997}, manifold exploration~\cite{Jakob2012}, gradient-domain path tracing~\cite{Kettunen2015}, etc. The book by~\citet{Pharr2017} gives a good overview of different path tracing techniques. We described one of these importance sampling techniques in Section~\ref{sec:volumept}, where we derived a formulation for interactive volume path tracing.

So when we need interactivity, different approximations can be used. Once we know the underlying physical process, different choices can be made on a per material basis. For example, in the case of most metals, the subsurface scattering of light is very limited around the point of incidence, so the BRDF approximation described in Section~\ref{sec:brdfsec} can be used. In the case of translucent materials, we can use the analytical BSSRDFs described in Section~\ref{sec:analyticalbssrdf} instead of a fully path-traced simulation. Depending on the directionality of the scattering effects, different models can be used, at the price of increased rendering times. Moreover, for some particular material, single scattering can be approximated by a BRDF, even for layered materials~\cite{Blinn1982, Hanrahan1993}. In applications such as games or virtual reality, often it is not even possible to use BSSRDF models, using an approximate BRDF instead. This gives acceptable results, but since subsurface scattering effects are important, this results in a "waxy" effect, in particular for materials like skin. 

Another approach, instead of simplifying the physical model, real measured data can be used instead. This is the approach for example used by the discretized BRDF described in Section~\ref{sec:empiricalbrdf}. The drawback of measured materials is that they often require large storage spaces. This is often a problem, since most rendering pipelines are memory-bound. Discretized models often have discretization issues, and require particular care in deciding a proper storage space, such as the Rusinkiewicz parameterization for measured BRDFs~\cite{Rusinkiewicz1998}. Finally, measured data are limited to the setup used to generate them. Tabulation of the BSSRDF was attempted by~\citet{Donner2009}, where the BSSRDF is simulated as a Monte Carlo path traced simulation. Many assumptions are required to simulate the BSSRDF in reasonable times, reducing the fourteen dimensions of the BSSRDF to a more tractable five.  

So far we have discussed material simplification. In general, the light hierarchy of a scene can also be object of simplification. Generically, a light is an object like any other in the scene, but it also emits light in the visible spectrum. In a path tracing context, lights are easily handled by the $L_e$ term in Equation~\ref{eq:renderingequation}. However, since the number of lights is usually limited, and since the light position is generally known in advance, multiple techniques can be used to improve convergence~\cite{Shirley1996}. Further simplifications are possible: in games and real-time applications, lights often do not have an area extent. This is the case of directional, point and spot lights. This allows us to represent lights as delta functions and replace the integral over $\Omega^+$ in Equations~\ref{eq:brdfintegral} and~\ref{eq:bssrdfintegral} with a sum over all the lights in the scene, with each contribution multiplied by a visibility factor. Recently, real-time polygonal lights have been introduced by~\citet{Heitz2016}.

We will briefly touch upon geometry and how it is represented in interactive applications. Various representations and primitives exists. However, in the vast majority of interactive application triangular meshes are employed. This comes from the fact that rasterization and ray tracing algorithms can be greatly optimized by assuming a unique type of planar primitive. Though GPUs are able to push more and more triangles due to improved hardware, excessive geometric detail still needs to be reduced in order to maintain acceptable frame rates. This is particularly true of massive scenes, where techniques as occlusion culling and level-of-detail~\cite{Clark1976} need to be employed to achieve interactive frame rates. 

\section{Rendering techniques}
%
Once we have made our choice of physical model, as we discussed in the previous section,  some other choices need to be done on the implementation side. These implementation choices account for the fact that our algorithm runs on a discrete system with finite memory and processing power. In this thesis, we focus on GPU techniques, which thus usually exploit the massive parallelism offered by the streamed multiprocessing units on GPUs. In literature, we identified three common approaches employed by the various techniques, namely caching, precomputation and filtering. Each of these approaches leverages some approximations or introduces some limitations in order to work. In caching, we use some intermediate data structures to accelerate the per-frame rendering time. Caching also implies using data structure to efficiently reuse data, either spatially (to exploit cache coherence) or temporally, to amortize computation across frames. Precomputation, the second approach, allows to move some of the computation before the program actually executes, given some assumptions about the materials or geometry. Finally, filtering, the final approach, reconstructs missing information based on a sparse sampling of a target function. Note that techniques often fall into multiple approaches: each technique is usually a combination of caching, precomputation and filtering. Bearing this in mind, we will now proceed to present relevant theory for each approach. 

%
\subsection{Caching}

In many cases in rendering, we need knowledge of both the local geometry around a geometric point (e.g. to estimate the occlusion of a point) in the scene and the overall global geometry (e.g. for global lighting effects). A number of techniques provide data structures to efficiently retrieve both the local and global geometry of a point. The most simple and widely used of these techniques is deferred shading~\cite{Saito1990}, which rasterizes geometry, depth and positions into a highly optimized screen space data structure. This structure allows sampling of local geometry, and it can be used to implement various screen space techniques. Multiple G-buffers~\cite{Mara2016} can be also be used to achieve global effects. For global effects, a common approach is to create a traversal structure to efficiently implement ray tracing, such as the bounding volume hierarchies described in Section~\ref{sec:raytracing}. Many optimized ray tracing techniques exploiting BVHs have been developed in recent years~\cite{Parker2010,Wald2014,Hendrich2017,Meister2018}. We use these data structures through the interfaced exposed by the OptiX programming system~\cite{Parker2010} to implement efficient path tracing in out Contributions~\ref{sec:juice} and~\ref{sec:glass} (see Section~\ref{sec:definingphoto}). Once a traversal structure for ray tracing is in place, various classical ray tracing algorithms can be efficiently implemented, such as recursive ray tracing, path tracing, or volumetric path tracing. Davidovi\v{c} et al.~\cite{Davidovic2014} provide a good survey about progressive path tracing techniques in a GPU context. Other data structures can be used, such as octrees~\cite{Glassner1988,Havran2000}, or trace rays directly in a screen space structure~\cite{Tanaka1986,McGuire2014, Widmer2015}. 

Other techniques involve some form of light caching. One of the simplest light caching techniques, the render cache~\cite{Walter2002}, can be done in screen space. This technique stores the radiance from a frame and reprojects it to the next, then filling holes created through visibility mismatches. Our Contribution~\ref{sec:srt} on stable ray tracing is within this area of research (see Section~\ref{sec:srtcontribution}). Screen space techniques can be used to efficiently render also scattering media~\cite{Nalbach2014}, by solving the extended rendering equation~\ref{eq:bssrdfintegral} by sampling geometry in a local neighborhood. Other light caching techniques propagate illumination in the scene, store it, then render the scene again from the camera. This allows to achieve global effects. Photon mapping~\cite{Jensen1996} is a seminal paper in this regard. In this technique, photons are cast from light sources, bounced around the scene though ray tracing, then arranged into photon maps. Density estimation is then use to derive the final illumination from the maps. The overall efficiency can be enhanced by building a data structure for fast gathering of nearby photons. See the paper by Mara et al.~\cite{Mara2013} for a fast GPU implementation. Instant radiosity~\cite{Keller1997} uses a similar approach, but uses gathering instead of density estimation in order to estimate the final radiance value at the exit point. These techniques are often named VPL (virtual point light) techniques, since each photon stored in the scene is treated as a small point light. Various literature deals with improving VPLs, including using the pixels in shadow map G-buffer as VPL sources, in the form of a reflective shadow map~\cite{Frisvad2005,Dachsbacher2005}. Other enchancements include VPL clustering~\cite{Walter2005, Bus2015}, adding visibility~\cite{Ritschel2008}, generalizing them into virtual area lights~\cite{Dong09}, or by enhancing them to store multiple views~\cite{Simon2015}. A comprehensive survey on VPL techniques is available at~\cite{Dachsbacher2014}. Our Contribution~\ref{sec:interactivedirsss} uses standard VPLs to transport outgoing scattered light (see Section~\ref{sec:interactivedirssscontribution}).

Mixed techniques that combine geometry and light approximation are also possible. One example is radiosity~\cite{Goral1984}, where we approximate the scene as a collection of geometric patches, then precompute the light transport in between patches. After this, the overall light transport problem can be described as solving a linear system. Another approach that employs both geometry simplification and light caching is point based global illumination~\cite{Bunnel2005, Christensen2008}, where the scene is represented as a series of surface elements (surfels). In this technique, we first build a hierarchy of surfels. Surfels are then shaded. Finally, for each pixel in the final rendering, the relevant surfels falling within that pixel are rendered, obtaining the final result. 

Other techniques use efficient data structures to do a volumetric light transport simulation. These techniques are particular suitable to render participating media, but they can also be used to render traditional diffuse and glossy illumination. In the case of participating media, these techniques often use finite elements to solve the radiative transfer equation on a discretized grid~\cite{Fattal2009}. These particular techniques are used to obtain real-time results, such as in the case of light propagation volumes~\cite{Kaplanyan2009,Borlum2011}, and further extended to discrete ordinate methods for point and directional lights~\cite{Elek2014}. In the case of diffuse and glossy illumination, various examples exist in the real time rendering community. Grid-based radiance caches can be used ~\cite{Nijasure2005}, and also combined with ideas from reflective shadow maps to obtain radiance hints~\cite{Papaioannou2011, Vardis2014}, that can handle multiple diffuse inter-reflections.  Crassin et al.~\cite{Crassin2011} propose voxel cone tracing using an adaptive octree data structure to filter and propagate lighting. This can then evaluated with cone rays. \citet{Hoetzlein2016} proposes and optimized GPU voxel based volumetric structure to visualize huge scientific datasets. 

\subsection{Pre-computation}
In this section, we deal with the aspect of precomputation. By introducing some limitation in our scene, we can precompute some data for efficient rendering. Limitations include static or mostly static geometry, static light, fixed materials or fixed cameras. 

Relighting techniques~\cite{Nimeroff94, Pellacini2005, Hasan2006} are a first example in which we require camera and geometry to be fixed. In this case, visibility and geometry are cached and reused across frames, allowing to interactively change lighting and materials. This is often necessary in a movie production pipeline, where due to artistic reasons lighting changes are more common than camera and geometry placement changes.

Precomputed radiance transfer~\cite{Sloan2002} assumes static geometry only (light and view can freely change). This family of techniques involve precomputing the radiance transfer at the surface for infinitely distant lights. Basis function are used to approximate lighting transfer at the surface, allowing slow preprocessing times but a fast evaluation via a dot product. The quality of the rendering depends on the number of basis functions used, though the memory and performance requirements then increase as well. We use this technique to approximate environment lighting in our Contribution~\ref{sec:vrbrdf}, as described in Section~\ref{sec:vrbrfcontribution}. Spherical harmonics are the most commonly used basis functions, but others such as Gaussians are possible~\cite{Green2006}. Precomputed radiance transfer gives good result representing low frequency lighting changes across the scene. The radiosity algorithm we described above is another algorithm that precomputes light transport assuming static geometry.

If we assume static light and geometry, the whole incoming illumination at a point can be cached, then used to re-light objects moving through the scene. Moreover, indirect illumination can be precomputed via ray tracing and stored as additional texture maps, called lightmaps. This technique has been widely used in games, starting from John Carmack's Quake in 1996.    

In scattering media, various precomputation are possible. We can also apply the precomputed radiance transfer technique, modified to work with translucent materials~\cite{Sloan2003}. Another approach is to precompute a grid, that can be used with a fast diffusion computation to render scattering in real-time~\cite{Wang2008a}. Finally, some methods preprocess the existing mesh to create a multi-resolution mesh that can be used to propagate irradiance via finite elements, and this can handle deformable objects at interactive framerates~\cite{Mertens2003, Li2013}. 

\subsection{Filtering}

The last branch of techniques we discuss is filtering. Filtering approaches regard reconstructing the final appearance based on a limited set of samples. As in previous sections, we start by describing screen space techniques. A big area of research includes anti-aliasing techniques, that involve improving appearance of undersampled features in the scene. Temporal antialiasing~\cite{Karis2014,Patney2016} involves recycling color information from the previous frame via motion vectors, and combining with the current color distribution, to achieve antialiasing across time. More recently, new techniques have been developed to filter noisy one sample Monte Carlo simulations to achieve a smooth temporally stable result. Work in this area include pre-filtering~\cite{Crassin2015}, advanced edge-aware bilateral filtering~\cite{Mara2017}, temporal variance averaging~\cite{Schied17}, and machine-learning based filtering~\cite{Chaitanya2017}. The work from Mehta et al.~\cite{Mehta2013} proposes an adaptive filtering scheme to efficiently filter out high frequency Monte Carlo noise respecting physically based constraints. We also contribute to this body of work with our Contribution~\ref{sec:srt}, described in Section~\ref{sec:srtcontribution}.

Moving into more advanced structures, irradiance caching techniques~\cite{Ward1988, Tole2002} store the illumination at a limited set of pixels, then interpolating illumination across the points to achieve overall results. The technique can be further enhanced to work in volumes~\cite{Greger1998}, and various heuristics in order to avoid light leaking though objects have been proposed~\cite{Gautron2009}. Irradiance caching has been extended to radiance caching to include directional information. Another set of techniques widely used in modern games is probe sampling~\cite{Levoy1996,Hooker2016, McGuire2017,Silvennoinen2017} where a spherical radiance map is stored at fixed points in the scene, called probes. The illumination is then interpolated across the probes to allow lighting of both static and dynamic geometries. 

Screen space techniques can be effectively used also in the case of scattering media, approximating the scattering process as a series of Gaussian filters~\cite{dEon2011, Jimenez2015}. This effectively approximates the diffusion process using the standard or better dipole described in Section~\ref{sec:analyticalbssrdf}, leading to plausible results. The path integral formulation~\cite{Premoze2003, Hegeman2005} and the narrow beam theory~\cite{Shinya2016} can be used instead of the diffusion approximation to propagate and filter the scattering contribution, giving interactive results.

As we have seen in this section, the space of interactive techniques that try to deliver a physically accurate result is huge. Most of these techniques rely on simplifications, assumptions and discretizations that allow fast rendering, often sacrificing physical accuracy in the process. In the next chapter, we will show how we improved upon current techniques to achieve a more physically accurate result.
