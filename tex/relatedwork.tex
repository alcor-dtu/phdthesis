\chapter{Related work}
\fixme{more connections to theory}

\fixme{connections to following chapter more clear}

\fixme{further explanations} 

\fixme{include references}

%
\label{sec:related}
%
In this section, we present some related work on the overall topic of the thesis, namely physically based techniques brought into the interactive domain. We will make use of the background theory we presented in Chapter~\ref{sec:background}, to better discuss our contributions in Chapter~\ref{sec:contributions}. This section is not meant to be a complete survey on such techniques, but we will point to the most important contributions, referring to a number of individual papers and surveys for a more complete overview. We also refer to the related work of the individual contributions, reported in the Appendix, for a more details in the context of the individual papers. 
%
As we mention in Chapter~\ref{sec:intro}, interactive rendering requires a compromise between accuracy and speed. There are various examples in literature on the compromises that are needed to hit this trade-off. Since the rendering time constraints are usually set, compromises are usually done in physical accuracy. We will start by discussing some of the assumptions that are usually employed in interactive rendering environments. 
%
\section{Approximate physically based rendering}

One of the natural approaches into achieving interactive photorealistic rendering is making some assumptions about our physical simulation. In principle, we can describe any light interaction by running a brute-force simulation of all the photons interacting with the particles of the material, then measuring the number of photons arriving at a set light sensor. Of course, running this sort of computation, even for extremely simple scenes, in reasonable times is not possible REF?. In general, Monte Carlo path tracing REF with extension to scattering media~\cite{Rushmeier1988} is considered a ground truth technique to generate reference images. Path tracing is an unbiased technique (i.e. converges always to the correct solution), but it gives extremely noisy results, especially for particular configurations of light and camera positions. To solve this problem, the offline rendering community proposed various improvements to reduce variance in path tracing, such as multiple importance sampling REF, Metropolis sampling REF, Bidirectional path tracing, MCMC methods, etc. The book by Pharr et al. REF gives a good overview of different path tracing techniques. We described one of these importance sampling techniques in Section TODO, where we derived a formulation for interactive volume path tracing.

So when we need interactivity, different approximations can be used. Once we know the underlying physical process, different choices can be made on a per material basis. For example, in the case of most metals, the scattering of light is fairly limited around the point of incidence, so the BRDF approximation described in section TODO can be used. In the case of scattering materials, we can use the analytical BSSRDFs described in Section TODO instead of a fully path traced simulation. Depending on the directionality of the scattering effects, different models can be used, at the price of increased rendering times. In applications such as games or virtual reality, often it is not even possible to use BSSRDF models, using an approximate BRDF instead. This gives acceptable results, but since scattering effects are important, this result in a "waxy" effect. This is particular noticeable in the case of skin. 

Another approach, instead of simplifying the physical model, real measured data can be used instead. This is the approach for example used by the discretized BRDF described in section TODO. The drawback of measured materials is that they often require large storage spaces. This is often a problem, since most rendering pipelines are memory-bound. Discretized models often have discretization issues, and require particular care in deciding a proper storage space, such as the Runsinkewitz parameterization for measured BRDF. Finally, measured data are limited to the setup used to generate them. In the simulated discretized BSSRDF by REF, many assumptions, including planarity of the configuration, are required to reduce the 14 dimensions of the BSSRDF to a more tractable 5.  

So far we have discussed material simplification. In general, the light hierarchy of a scene can also be object of simplification. Generically, a light is an object like any other in the scene, but that also emits light in the visible spectrum. This can be handled by adding the $L_e$ term in equations TODO and TODO, so that lights can be easily added to any path tracing framework. However, since the position of the lights is generally known in advance, multiple techniques can be used to importance sample the lights. Further simplifications are possible: in games and real time applications, lights often do not have an area extent. This is the case of directional, point and spot lights REF. This allows to represent lights as delta functions and replace the integral over $\Omega^+$ in Equations TODO and TODO with a sum over all the lights in the scene. Recently, real-time polygonal lights have been introduced by REF.

One final simplification we discuss in interactive applications is geometry simplification. Tough GPUs are able to push more and more triangles out, sometimes the scene needs to be simplified in order to maintain acceptable frame rate. This is particularly true of massive scenes, where techniques as occlusion culling and LODs need to be employed to achieve interactive framerates. 

\section{Interactive rendering techniques}
%
Once we have made our choice of physical model, as we discussed in the previous section,  some other choices need to be done on the implementation side. These implementation choices account for the fact that our algorithm runs on a discrete system with finite memory and processing power. In this thesis, we focus on GPU techniques, that thus usually exploit the massive parallelism offered by the streamed multiprocessing units on GPUs. In literature, we identified three common approaches the various techniques employ, namely caching, precomputation and filtering. Each of these approaches leverages some approximations or introduces some limitations in order to work. In caching, we use some intermediate data structures to accelerate the per-frame rendering time. Caching also implies using data structure to efficiently reuse data, either spatially (to exploit cache coherence) or temporally, to amortize computation across frames. Precomputation, the second approach, allows to move some of the computation before the program actually executes, given some assumptions about the materials or geometry. Finally, filtering, the final approach, reconstructs missing information based on a sparse sampling of a target function. Note that techniques often fall into multiple approaches: each technique is usually a combination of caching, precomputation and filtering. Bearing this in mind, we will now proceed to present relevant theory for each approach.
%
\subsection{Caching}

In many cases in rendering, we need knowledge of both the local geometry around a geometric point (e.g. to estimate the occlusion of a point) in the scene and the overall global geometry (e.g. for global lighting effects). A number of techniques provide efficient data structures to efficiently retrieve both the local and global geometry of a point. The most simple and widely used of these techniques is deferred shading REF, that rasterizes geometry, depth and positions into a highly optimized screen space data structure. This structure allows sampling of local geometry, and it can be used to implement various screen space techniques REF. For global effects, a common approach is to create a traversal structure to efficiently implement ray tracing, such as the bounding volume hierarchies described in section TODO. Data structures for interactive ray tracing is a field of itself, so we point out to a relevant survey by REF for further reading REF. Once a traversal structure for ray tracing is in place, various classical ray tracing algorithms can be efficiently implemented, such as recursive ray tracing, path tracing, or volumetric path tracing. Other data structures can be used, such as octrees REF, or trace rays directly in the G-buffer, if available REF.

Other techniques involve some form of light caching. One of the simplest light caching techniques, the radiance cache REF, can be done in screen space. This techniques stores the radiance from a frame and reprojects it to the next, then filling holes created through visibility mismatches. Screen space techniques can be used to efficiently render also scattering media, by solving the extended rendering equation REF by sampling geometry in a local neighborhood. Other light caching techniques propagate illumination in the scene, store it, then render the scene again from the camera. This allows to achieve global effects. Photon mapping REF is a seminal paper in this regard. In this technique, photons are cast from light sources, bounced around the scene though ray tracing, then arranged into photon maps. Density estimation is then use to derive the final illumination from the maps. The overall efficiency can be enhanced by building a data structure for fast gathering of nearby photons. Instant radiosity techniques REF use a similar approach, but use gathering instead of density estimation in order to estimate the final radiance value at the exit point. These techniques are often named VPL (virtual point light) techniques, since each photon stored in the scene is treated as a small point light. A lot of literature deals with eliminating VPL singularities REF, adding visibility REF or generalizing them into virtual area lights REF.

Mixed techniques that combine geometry and light approximation are also possible. One example is radiosity, where we approximate the scene as a series of geometric patches, then precompute the light transport in between patches. After this, the overall light transport problem can be described as solving a linear system. Another approach that employs both geometry simplification and light caching is point based global illumination, where the scene is represented as a series of surface elements (surfels). In this technique, we first build a hierarchy of surfels. Surfels are then shaded. Finally, for each pixel in the final rendering, the relevant surfels falling within that pixel are rendered, obtaining the final result. 

Other techniques use efficient data structures to do a light transport simulation. These techniques are particular suitable to render participating media, but they can also be used to render traditional diffuse and glossy illumination. In the case of participating media, these techniques often use finite elements to solve the radiative transfer equation on a discretized grid. Various improvements can be applied, such as using a adaptive octree data structure to propagate lighting. This particular techniques are used to obtain real time results, such as in the case of light propagation volumes REF or voxel cone tracing REF. 

\subsection{Pre-computation}
In this section, we deal with the aspect of precomputation. By introducing some limitation in our scene, we can precompute some data for efficient rendering. Limitations include static or mostly static geometry, static light, fixed materials or fixed cameras. 

Relighting techniques REF are a first example in which we require camera and geometry to be fixed. In this case, we precompute and change lighting in the scene. These techniques are suitable in a preview environment, or to perform artistic tweaks in the final rendering. REF is an example of this technique used in movie rendering pipelines. 

Precomputed radiance transfer techniques REF assume static geometry only (light and view can freely change). This family of techniques involve precomputing the radiance transfer at the surface for infinitely distant lights. Basis function are used to approximate lighting transfer at the surface, allowing slow preprocessing times but a fast evaluation via a dot product. The quality of the rendering depends on the number of basis functions used, though the memory and performance requirements then increase as well. Spherical harmonics are the most commonly used basis functions REF, but more complex ones are possible REF. Precomputed radiance transfer gives good result representing low frequency lighting changes across the scene. Still in the case of techniques that require static geometry, radiosity discretizes the scene as a series of patches, then calculates the transfer function between each patch. This allows fast illumination with changing lights and viewpoints, at the price of a quadratic memory increase depending on the number of patches. 

if we assume static light and geometry, the whole incoming illumination at a point can be cached, then used to re-light objects moving through the scene. Moreover, static illumination can be stored as an additional texture map (baking), with direct illumination added on top. 

\subsection{Filtering}

The last branch of techniques we discuss is filtering. Filtering approaches regard reconstructing the final appearance based on a limited set of samples. As in previous sections, we start by describing screen space techniques. A big area of research includes anti-aliasing techniques, that involve improving appearance of undersampled features in the scene. Temporal antialiasing techniques involve recycling color information from the previous frame via motion vectors, and combining with the current color distribution, to achieve antialiasing across time. More recently, new techniques have been developed to filter noisy one sample Monte Carlo simulations to achieve a smooth temporally stable result REF. Work in this area include advanced edge-aware bilateral filtering REF, temporal variance averaging REF, and machine-learning based filtering REF. Screen space techniques can be effectively used also in the case of scattering media, approximating the scattering process as a series of Gaussian filters. This effectively approximates the diffusion process using the standard dipole describe in Section TODO REF, leading to plausible realistic results. 

Moving into more advanced structures, irradiance caching techniques store the illumination at a limited set of points in the scene, or pixels, then interpolating illumination across the points to achieve overall results. The technique can be further enhanced with various heuristics in order to avoid light leaking though objects. Another technique in global space is probe sampling, where precomputed radiance transfer is stored at fixed points in the scene, then the illumination is interpolated across the probes to allow lighting of both static and dynamic geometries. Interpolation can be done in many ways, including Voronoi 3D grids to allow arbitrary placements of probes.

As we have seen in this section, the space of interactive techniques that try to deliver a physically accurate result is huge. Most of these techniques rely on simplifications, assumptions and discretizations that allow fast rendering, often sacrificing physical accuracy in the process. In the next chapter, we will show how we improved upon current techniques to achieve a more physically accurate result.
