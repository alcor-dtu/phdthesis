\chapter{Related work}
\fixme{more connections to theory}

\fixme{connections to following chapter more clear}

\fixme{further explanations} 

\fixme{include references}

%
\label{sec:related}
%
In this section, we present some related work on the overall topic of the thesis, namely physically based techniques brought into the interactive domain. We will make use of the background theory we presented in Chapter~\ref{sec:background}, to better discuss our contributions in Chapter~\ref{sec:contributions}. This section is not meant to be a complete survey on such techniques, but we will point to the most important contributions, referring to a number of individual papers and surveys for a more complete overview. We also refer to the related work of the individual contributions, reported in the Appendix, for a more details in the context of the individual papers. 
%
As we mention in Chapter~\ref{sec:intro}, interactive rendering requires a compromise between accuracy and speed. There are various examples in literature on the compromises that are needed to hit this trade-off. Since the rendering time constraints are usually set, compromises are usually done in physical accuracy. We will start by discussing some of the assumptions that are usually employed in interactive rendering environments. 
%
\section{Simplified physical models}
One of the main techniques used in interactive rendering is some sort of simplification of the physical models. In principle, we can describe any light interaction by running a bull simulation of all the photons hitting all the particles of the material. Of course, running this sort of computation at interactive speed even for very simple scenes is not  possible. In general, Monte Carlo path tracing with extension to scattering media is considered a ground truth unbiased technique to generate reference images. The problem with path tracing is that it is extremely noisy, that it is often unacceptable for interactive applications.

So we require some sort of simplification in order to make interactive rendering happen. For example, analytical BSSRDFs models can be used instead of path tracing. This leads to a biased result, whose quality it is still acceptable. In general, BRDF models can be used instead of BSSRDF models: this works under the assumptions listed in Section TODO, but it fails miserably when applied to translucent media such as skin. This is why skin may appear "waxy" in real time rendering applications where a BRDF is used in place of a BSSRDF. 

So far we talked about material simplification. In general, lights often receive the same treatment: lights in general have an area extent, while in games they are often approximated as directional or point lights. This allows to represent lights as delta functions and remove the directional integral in Equations TODO and TODO. In recent years, real time polygonal lights have been introduced TODO. Often, only a certain amount of lights is allowed. 

One final simplification that often happens in interactive applications is geometry simplification. Tough GPUs are able to push every year more and more triangles out, sometimes the scene needs to be simplified in order to maintain acceptable frame rate. This is often artist driven, tough some automatic algorithms exist, in particular for LODs and terrain.   

\section{Interactive rendering techniques}
%
After defining the approximation and compromises usually employed on the physical model side, some other choices need to be done on the implementation side. These choices exploit the fact that our algorithm runs on a discrete system with finite memory. In this thesis, we focus on GPU techniques, that thus have to exploit the massive parallelism offered by the streamed multiprocessing units on the chip. We identified three common approaches which the various techniques employ to achieve interactive rendering: caching, precomputation and filtering. Each of these approaches leverages some approximations or limitations in order to work. In caching, we use some intermediate data structures to accelerate the average computation  time in between all pixels. Caching also implies using data structure to efficiently reuse data, either spatially (to exploit cache coherence) or temporally, to amortize computation across frames. The second technique, given some assumptions about the materials or geometry, allows to move some of the computation before the program actually executes. Finally, filtering techniques reconstruct missing information based on a sparse sampling of a target function. Note that the techniques we are going to describe often fall into multiple approaches: each technique may imply a combination of caching, precomputation and filtering. We will highlight when this happen, choosing to describe a technique within the approach that summarizes it best.
%
\subsection{Caching}

The caching approach involves creating data structures to store additional data in a rendered scene, to make rendering faster. We can subdivide the techniques in this area into two broad categories: caching geometry and caching lighting.

A number of techniques caching geometry provide efficient data structures to traverse the scene. The most simple and widely used of these techniques is deferred shading REF, that rasterizes geometry, depth and positions into a highly optimized screen space data structure. This structure allows sampling of geometry in a local neighborhood and can be used to implement various screen space techniques, such as ambient occlusion. Another technique is to create a traversal structure to efficiently implement ray tracing, as the bounding volume hierarchies described in section TODO. Data structures for interactive ray tracing is a field by itself, so we point out to a relevant survey by REF. In particular, we want to point out the paper by Karras [2013], the first to introduce the TRBVH, a particular structure that allows fast rebuilding of the BVH on the fly, and that it is implemented in OptiX, NVIDIA's ray tracing engine REF. Once a traversal structure for ray tracing is in place, various classical ray tracing algorithms can be efficiently implemented, such as recursive ray tracing, path tracing, or volumetric path tracing. Other data structures can be used, such as octrees REF or trace rays directly in the G-buffer, if available REF.

When we consider light caching techniques, we also have screen space techniques. These involve storing the radiance from a frame and reprojecting it to the next, then filling holes created through visibility mismatches. The radiance cache REF is a seminal paper in this regard. Screen space techniques can be used to efficiently render also scattering media, by solving the extended rendering equation REF by sampling geometry in a local neighborhood.

Other light caching techniques propagate illumination in the scene, store it, then render the scene again from the camera. In photon mapping REF, photons are cast from light sources, bounced around the scene though ray tracing, then arranged into photon maps. Density estimation is then use to derive the illumination from the maps. The overall efficiency can be enhanced by building a data structure for fast gathering of nearby photons. Instant radiosity techniques use a similar approach, but use gathering instead of density estimation in order to estimate the final radiance value at the exit point. Each photon stored in the scene is treated as a virtual point light (VPL). A lot of literature deals with eliminating VPL singularities REF, adding visibility REF or generalizing them into virtual area lights REF.

Mixed techniques that combine geometry and light approximation are also possible, as in point based global illumination, where the scene is represented as a series of either points or surface elements (surfels). A hierarchy of surfels is then built. Surfels are then shader, then relevant ones are then rendered per each pixel, resolving indirect visibility. Other techniques such as radiosity approximate the scene as a series of geometric patches, then precomputes the light transport in between patches for subsequent interactive rendering. 

Other techniques use efficient data structures to do a light transport simulation. These techniques are particular suitable to render participating media, but they can also be used to render traditional diffuse and glossy illumiation. In the case of participating media, these techniques use finite element technique to solve the radiative transfer equation on a discretized grid. Various improvements can be applied, such as using a adaptive octree data structure to propagate lighting. This particular techniques are used to obtain real time results, such as in the case of light propagation volumes REF or voxel cone tracing. 

\subsection{Pre-computation}
In this section, we deal with the aspect of precomputation. By introducing some limitation in our scene, we can precompute some data for efficient rendering. Limitations include static or mostly static geometry, static light, fixed materials or fixed view point. 

Relighting techniques REF are a first example in which we require camera and geometry to be fixed. In this case, we precompute and change lighting in the scene. These techniques are suitable in a preview environment, or to perform artistic tweaks in the final rendering. REF is an example of this technique used in movie rendering pipelines. 

If we assume static geometry only (light and view can freely change), one of the most important techniques is precomputed radiance transfer. This echnique involve precomputing the radiance transfer at the surface for infinitely distant lights. Basis function are used to approximate lighting transfer at the surface, allowing slow preprocessing times but a fast evaluation via a dot product. The quality of the rendering depends on the number of basis functions used, though the memory and performance requirements increase as well. Spherical harmonics are the most commonly used basis functions REF, but more complex ones are possible. Precomputed radiance transfer gives good result representing low frequency lighting changes across the scene. Still in the case of techniques that require static geometry, radiosity discretizes the scene as a series of patches, then calculates the transfer function between each patch. This allows fast illumination with changing lights and viewpoints, at the price of a quadratic memory increase depending on the number of patches. 

if we assume static light and geometry, the whole incoming illumination at a point can be cached, then used to re-light objects moving through the scene. Moreover, static illumination can be stored as an additional texture map, in a process called baking. Illumination can then be used in rendering to represent complex effect, with direct illumination added on top. 

\subsection{Filtering}

The last branch of techniques we discuss is filtering. Filtering approaches regard reconstructing the final appearance based on a limited set of samples. As in previous sections, we start by describing screen space techniques. A big area of research includes anti-aliasing techniques, that involve improving appearance of undersampled features in the scene. Important technique in this area include MSAA REF to improve geometric aliasing and other work REF to improve aliasing of specular. Temporal antialiasing techniques involve recycling color information from the previous frame via motion vectors, and combining with the current color distribution, to achieve antialiasing across time. More recently, new techniques have been developed to filter noisy one sample Monte Carlo simulations to achieve a smooth temporally stable result. Work in this area include advanced edge-aware bilateral filtering, temporal variance averaging, and machine-learning based filtering. Screen space techniques can be effectively used also in the case of scattering media, approximating the scattering process as a series of Gaussian filters. This effectively approximates the diffusion process using the standard dipole REF, leading to plausible realistic results. 

Moving into more advanced structures, irradiance caching techniques store the illumination at a limited set of points in the scene, or pixels, then interpolating illumination across the points to achieve overall results. The technique can be further enhanced to include spatial heuristics in order to avoid light leaking though objects. Another techcnique in global space is probe sampling, where precomputed radiance transfer is stored at fixed points in the scene, then the illumination is interpolated across the probes to allow lighting of both static and dynamic geometries. Interpolation can be done in many ways, including Voronoi 3D grids to allow arbitrary placements of probes.

As we have seen in this section, the space of interactive techniques that try to deliver a physically accurate result is huge. Most of these techniques rely on simplifications, assumptions and discretizations that allow fast rendering, often sacrificing physical accuracy in the process. In the next chapter, we will show how we improved upon current techniques to achieve a more physically accurate result.
